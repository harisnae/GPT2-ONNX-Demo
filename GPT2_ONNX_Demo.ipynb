{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB5uMEbTZW6U"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 1.  System packages (run once ‚Äì nothing special for this model)\n",
        "# ==============================================================\n",
        "\n",
        "!apt-get -qq update && apt-get -qq install -y git wget   # quiet apt update, install git & wget (useful for debugging)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 2.  Install the *latest* Python dependencies (run once)\n",
        "#    ‚Äì tokenizer + generation utilities\n",
        "#    ‚Äì Optimum wrapper + ONNX‚ÄëRuntime integration\n",
        "# ==============================================================\n",
        "\n",
        "# NOTE:\n",
        "#   * We install the newest releases of `transformers` and `optimum`.\n",
        "#   * `optimum` (v1.16.0+) has been updated to work with the latest\n",
        "#     `transformers` where `cached_property` was moved to `functools`.\n",
        "#   * If you ever run into the same import error again, the small\n",
        "#     monkey‚Äëpatch below will fix it for you.\n",
        "!pip install -q --upgrade \\\n",
        "    \"transformers\" \\\n",
        "    \"huggingface_hub\" \\\n",
        "    \"optimum[onnxruntime]\" \\\n",
        "    \"sentencepiece\"   # needed for some tokenizers\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 3.  Install the correct ONNX Runtime build (CPU or GPU)\n",
        "# ==============================================================\n",
        "\n",
        "import sys, subprocess, os, torch, numpy as np # Import necessary modules\n",
        "\n",
        "def pip_install(pkgs): # Define a helper function for quiet pip installs\n",
        "    \"\"\"Quiet pip install in a subprocess.\"\"\"\n",
        "    # Execute pip install command silently in a subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
        "\n",
        "# Detect whether a CUDA device is available (torch is already installed by Optimum)\n",
        "cuda_available = torch.cuda.is_available() # Check if a CUDA-enabled GPU is available\n",
        "print(f\"CUDA available: {cuda_available}\") # Print the CUDA availability status\n",
        "\n",
        "# Install the matching ONNX‚ÄëRuntime package\n",
        "if cuda_available: # If CUDA is available\n",
        "    pip_install([\"onnxruntime-gpu\"])   # Install the GPU-enabled ONNX Runtime\n",
        "elif 'onnxruntime' not in sys.modules: # Else if ONNX Runtime is not already installed\n",
        "    pip_install([\"onnxruntime\"])       # Install the CPU-only ONNX Runtime\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 4.  Compatibility patch for newer `transformers`\n",
        "# ==============================================================\n",
        "\n",
        "# Newer versions of ü§ó‚ÄØTransformers (‚â•‚ÄØ4.36) removed the\n",
        "# `transformers.utils.cached_property` helper.  Optimum (and some\n",
        "# older user code) still expects it, so we add a tiny shim that\n",
        "# points to the standard library implementation.\n",
        "import transformers # Import the transformers library\n",
        "if not hasattr(transformers.utils, \"cached_property\"): # Check if cached_property is missing from transformers.utils\n",
        "    from functools import cached_property as _cached_property # Import cached_property from functools\n",
        "    transformers.utils.cached_property = _cached_property # Assign it to transformers.utils\n",
        "    print(\"Patched transformers.utils.cached_property ‚Üí functools.cached_property\") # Print confirmation of patch\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 5.  Download the GPT‚Äë2 ONNX repository from the Hub\n",
        "# ==============================================================\n",
        "\n",
        "from huggingface_hub import snapshot_download # Import snapshot_download for downloading Hugging Face repos\n",
        "\n",
        "repo_id = \"onnx-community/gpt2-ONNX\"   # HF repo that contains the exported ONNX files (Define the Hugging Face repository ID)\n",
        "\n",
        "# Keep only the files we actually need ‚Äì this speeds up the download\n",
        "onnx_dir = snapshot_download( # Download a snapshot of the repository\n",
        "    repo_id=repo_id, # Specify the repository ID\n",
        "    allow_patterns=[ # Specify patterns for files to include in the download\n",
        "        \"*.onnx\",               # all ONNX model files (model, fp16, q4, q4f16, ‚Ä¶)\n",
        "        \"*_data\",               # external weight blobs referenced by the .onnx files\n",
        "        \"config.json\", # Configuration file\n",
        "        \"generation_config.json\", # Generation configuration file\n",
        "        \"tokenizer_config.json\", # Tokenizer configuration file\n",
        "        \"special_tokens_map.json\", # Special tokens mapping\n",
        "        \"added_tokens.json\", # Added tokens file\n",
        "        \"vocab.json\", # Vocabulary file\n",
        "        \"merges.txt\", # Merges file for BPE tokenizers\n",
        "        \"tokenizer.json\", # Tokenizer definition file\n",
        "        \"tokenizer.model\",      # SentencePiece vocab (if present)\n",
        "    ],\n",
        "    local_dir=\"./gpt2-ONNX\",   # where the repo will be stored locally (Set the local directory for download)\n",
        "    cache_dir=\"./hf_cache\",    # shared cache folder (speeds up re‚Äëruns) (Set the cache directory)\n",
        "    resume_download=True, # Enable resuming interrupted downloads\n",
        ")\n",
        "print(\"Repo downloaded to:\", onnx_dir) # Print the local path where the repository is downloaded\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 6.  Load tokenizer & ONNX‚Äëruntime‚Äëbacked model (Optimum)\n",
        "# ==============================================================\n",
        "\n",
        "from transformers import AutoTokenizer, GenerationConfig # Import GPT2TokenizerFast and GenerationConfig from transformers\n",
        "from optimum.onnxruntime import ORTModelForCausalLM # Import ORTModelForCausalLM from optimum.onnxruntime\n",
        "\n",
        "# Tokenizer files live at the repo root, so we can point the tokenizer directly at `onnx_dir`\n",
        "tokenizer = AutoTokenizer.from_pretrained(onnx_dir) # Load the tokenizer from the downloaded ONNX directory\n",
        "\n",
        "# Choose the execution provider list\n",
        "providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if cuda_available else [\"CPUExecutionProvider\"] # Determine ONNX Runtime providers based on CUDA availability\n",
        "\n",
        "# Load the ONNX model.  `file_name` defaults to `model.onnx` (full‚Äëprecision).\n",
        "# You can switch to a quantised version (e.g. \"model_q4.onnx\") by changing the argument.\n",
        "model = ORTModelForCausalLM.from_pretrained( # Load the ONNX model for causal language modeling\n",
        "    onnx_dir, # Specify the directory containing the ONNX model files\n",
        "    file_name=\"model.onnx\",          # change to \"model_q4.onnx\", \"model_fp16.onnx\", ‚Ä¶ if you want a smaller model (Specify the ONNX model file name)\n",
        "    provider=providers[0]            # Optimum expects a *single* provider string (Set the execution provider)\n",
        ")\n",
        "\n",
        "# Load generation defaults if the repo ships a generation_config.json\n",
        "gen_cfg_path = os.path.join(onnx_dir, \"generation_config.json\") # Construct the path to generation_config.json\n",
        "if os.path.isfile(gen_cfg_path): # Check if the generation config file exists\n",
        "    generation_config = GenerationConfig.from_pretrained(onnx_dir) # Load generation config from the directory\n",
        "else: # If the generation config file does not exist\n",
        "    generation_config = GenerationConfig()   # empty fallback (Create an empty GenerationConfig)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 7.  Simple generation wrapper\n",
        "# ==============================================================\n",
        "\n",
        "import textwrap # Import the textwrap module for text formatting\n",
        "\n",
        "def generate_text( # Define the text generation function\n",
        "    prompt: str, # Input prompt string\n",
        "    max_new_tokens: int = 50, # Maximum number of new tokens to generate\n",
        "    temperature: float = 0.8, # Sampling temperature for controlling randomness\n",
        "    top_k: int = 50, # Number of top-k tokens to consider for sampling\n",
        "    stop_token: str | None = None, # Optional token to stop generation at\n",
        "    wrap_width: int | None = 80, # Added new parameter for text wrapping width\n",
        "    **extra_kwargs, # Additional keyword arguments for model.generate\n",
        ") -> str: # Function returns a string\n",
        "    \"\"\"\n",
        "    Generate text using the ONNX‚Äëruntime‚Äëbacked GPT‚Äë2 model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prompt : str\n",
        "        Text to condition on.\n",
        "    max_new_tokens : int\n",
        "        Number of tokens to generate (excluding the prompt).\n",
        "    temperature : float\n",
        "        Sampling temperature (>0). 1.0 = no scaling.\n",
        "    top_k : int\n",
        "        Keep only the top‚Äëk tokens at each step (0 = keep all).\n",
        "    stop_token : str | None\n",
        "        If given, generation stops when this token appears in the output.\n",
        "    wrap_width : int | None\n",
        "        If given, the output text will be wrapped to this many characters.\n",
        "    extra_kwargs : dict\n",
        "        Any additional arguments accepted by `model.generate`\n",
        "        (e.g. `num_beams=4`, `repetition_penalty=1.2`, ‚Ä¶).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        The full decoded text (prompt + continuation).\n",
        "    \"\"\"\n",
        "    # Tokenise the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")   # shape (1, seq_len) (Encode the prompt into input IDs)\n",
        "\n",
        "    # Build a GenerationConfig that overrides the defaults we care about\n",
        "    cfg = GenerationConfig(**generation_config.to_dict()) # Create a new GenerationConfig from the existing one's dictionary\n",
        "    cfg.max_new_tokens = max_new_tokens # Set the maximum number of new tokens\n",
        "    cfg.temperature = temperature # Set the sampling temperature\n",
        "    cfg.top_k = top_k # Set the top-k value for sampling\n",
        "    cfg.do_sample = temperature != 0.0   # greedy when temperature==0 (Enable sampling if temperature is not 0, else use greedy decoding)\n",
        "    # Apply any extra kwargs the user passed\n",
        "    for k, v in extra_kwargs.items(): # Iterate through extra keyword arguments\n",
        "        setattr(cfg, k, v) # Set each extra argument as an attribute of the generation config\n",
        "\n",
        "    # Run generation\n",
        "    output_ids = model.generate(input_ids, generation_config=cfg) # Generate output tokens using the model and configuration\n",
        "\n",
        "    # Decode everything (including the original prompt)\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True) # Decode the generated tokens into text\n",
        "\n",
        "    # Optional stop‚Äëtoken truncation\n",
        "    if stop_token is not None: # If a stop token is provided\n",
        "        idx = full_text.find(stop_token) # Find the index of the stop token\n",
        "        if idx != -1: # If the stop token is found\n",
        "            full_text = full_text[: idx + len(stop_token)] # Truncate the text at the stop token\n",
        "\n",
        "    # Apply text wrapping if wrap_width is specified\n",
        "    if wrap_width is not None: # If a wrap width is specified\n",
        "        full_text = textwrap.fill(full_text, width=wrap_width) # Wrap the text to the specified width\n",
        "\n",
        "    return full_text # Return the generated and formatted text\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "# 8.  Quick demo\n",
        "# ==============================================================\n",
        "\n",
        "prompt = \"Once upon a time in a distant galaxy\" # Define the prompt for text generation\n",
        "print(\"üñäÔ∏è Prompt:\", prompt) # Print the prompt\n",
        "print(\"\\nü§ñ Generation:\\n\") # Print a header for the generation output\n",
        "print( # Print the generated text\n",
        "    generate_text( # Call the generate_text function\n",
        "        prompt, # Pass the prompt\n",
        "        max_new_tokens=500, # Generate up to 80 new tokens\n",
        "        temperature=0.9, # Set sampling temperature to 0.9\n",
        "        top_k=50, # Consider top 50 tokens for sampling\n",
        "        stop_token=None,   # e.g. set to \".\" to stop at the first period (No stop token for this demo)\n",
        "        wrap_width=90, # Set a default wrap width for the demo (Wrap output to 80 characters)\n",
        "    )\n",
        ")"
      ]
    }
  ]
}